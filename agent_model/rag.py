import os
import glob
import time
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from sentence_transformers import SentenceTransformer


class RealTimeRAG:
    
    def __init__(self, 
                 embedding_model_name: str = "./Qwen3-0.6B-embedding",
                 chunk_size: int = 500,
                 chunk_overlap: int = 50,
                 persist_directory: str = "./multi_faiss_db"):
        """
        Args:
            embedding_model_name: SentenceTransformeræ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å‰²å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å‰²é‡å å¤§å°
            persist_directory: FAISSç´¢å¼•å­˜å‚¨æ ¹ç›®å½•
        """
        self.embedding_model_name = embedding_model_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.persist_directory = persist_directory
        
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        # os.makedirs(self.persist_directory, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
        
        # å­˜å‚¨çŠ¶æ€
        # self.vectorstores = {}  # æ–‡ä»¶å -> FAISSå‘é‡åº“
        # self.document_info = {}  # æ–‡ä»¶ä¿¡æ¯
        # self.is_loaded = False
        # self.document_count = 0
        # self.loaded_files = set()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        print("ğŸ”§ Initializing Episodic RAG components...")
        
        # 1. åŠ è½½embeddingæ¨¡å‹
        print(f"ğŸ”¢ Loading embedding model: {self.embedding_model_name}")
        start_time = time.time()
        self.embedding_model = SentenceTransformer(self.embedding_model_name)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        self.text_spliter = SimpleTextSplitter(self.chunk_size, self.chunk_overlap)
        end_time = time.time()
        print(f"âœ… Embedding model loaded in {end_time - start_time:.2f}s")
        print(f"ğŸ“Š Embedding dimension: {self.embedding_dim}")
        print("âœ… Real-time RAG components initialized successfully!")
    

    def execute(self, 
                query: str, 
                documents: List[str],
                k: int = 5,
                return_scores: bool = False,
                score_threshold: Optional[float] = None) -> Union[List[str], Tuple[List[str], List[float]]]:
        """
        æ‰§è¡Œæ£€ç´¢ï¼Œè¿”å›ä¸queryæœ€ç›¸ä¼¼çš„å‰kä¸ªæ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            return_scores: æ˜¯å¦è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
            score_threshold: åˆ†æ•°é˜ˆå€¼ï¼Œåªè¿”å›åˆ†æ•°é«˜äºè¯¥å€¼çš„æ–‡æ¡£
            
        Returns:
            å¦‚æœ return_scores=True: è¿”å› (documents, scores)
            å¦‚æœ return_scores=False: è¿”å› documents
        """
        if not documents:
            return ([], []) if return_scores else []
        
        if isinstance(query, str):
            query = [query]
            
        if self.text_spliter:
            chunks_list = []
            for doc in documents:
                chunks = self.text_spliter.split_text(doc)
                # print("doc:", doc)
                # print("chunk:", chunks)
                chunks_list += chunks
            documents = chunks_list
                    
        # 1. ç¼–ç queryå’Œdocuments
        query_embeddings = self.embedding_model.encode(query, prompt_name="query", normalize_embeddings=True)
        document_embeddings = self.embedding_model.encode(documents, normalize_embeddings=True)
        
        # 2. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆcosine similarityï¼‰
        # SentenceTransformerçš„similarityè¿”å›çš„æ˜¯cosineç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = np.matmul(query_embeddings, document_embeddings.T)
        # print(similarity_matrix, similarity_matrix.shape)
        
        # 3. è·å–queryä¸æ¯ä¸ªdocumentçš„ç›¸ä¼¼åº¦åˆ†æ•°
        # similarity_matrixå½¢çŠ¶ä¸º (1, num_documents)
        similarity_scores = similarity_matrix[0]
        
        # 4. æŒ‰åˆ†æ•°é™åºæ’åºï¼Œè·å–å‰kä¸ªç´¢å¼•
        if k > len(documents):
            k = len(documents)
        
        # è·å–å‰kä¸ªæœ€é«˜åˆ†çš„ç´¢å¼•
        top_k_indices = np.argsort(similarity_scores)[-k:][::-1]
        
        # 5. åº”ç”¨åˆ†æ•°é˜ˆå€¼ï¼ˆå¦‚æœæä¾›ï¼‰
        if score_threshold is not None:
            valid_indices = [idx for idx in top_k_indices if similarity_scores[idx] >= score_threshold]
            if not valid_indices:
                return ([], []) if return_scores else []
            top_k_indices = np.array(valid_indices)
        
        # 6. è·å–å¯¹åº”çš„æ–‡æ¡£å’Œåˆ†æ•°
        top_documents = [documents[idx] for idx in top_k_indices]
        top_scores = [float(similarity_scores[idx]) for idx in top_k_indices]
        
        # 7. è¿”å›ç»“æœ
        if return_scores:
            return top_documents, top_scores
        else:
            return top_documents
        
    
import re
from typing import List


class SimpleTextSplitter:
    """
    è½»é‡çº§æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆæ— é¢å¤–ä¾èµ–ï¼‰
    """
    
    def __init__(self, 
                 chunk_size: int = 500,
                 chunk_overlap: int = 50,
                 separators: List[str] = None):
        """
        Args:
            chunk_size: å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: é‡å å¤§å°
            separators: åˆ†éš”ç¬¦åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        if separators is None:
            # ä¸­æ–‡å‹å¥½çš„åˆ†éš”ç¬¦
            self.separators = [
                # "\n\n",      # åŒæ¢è¡Œï¼ˆæ®µè½ï¼‰
                # "\n",        # å•æ¢è¡Œ
                # "ã€‚",        # å¥å·
                # "ï¼",        # æ„Ÿå¹å·
                # "ï¼Ÿ",        # é—®å·
                # "ï¼›",        # åˆ†å·
                # "ï¼Œ",        # é€—å·
                # " ",         # ç©ºæ ¼
                # ""           # æœ€åæŒ‰å­—ç¬¦åˆ†å‰²
            ]
        else:
            self.separators = separators
    
    def split_text(self, text: str) -> List[str]:
        """
        åˆ†å‰²æ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text:
            return []
        
        # é€’å½’åˆ†å‰²å‡½æ•°
        def recursive_split(current_text: str, current_separators: List[str]) -> List[str]:
            # å¦‚æœæ–‡æœ¬å·²ç»è¶³å¤Ÿå°ï¼Œç›´æ¥è¿”å›
            if len(current_text) <= self.chunk_size:
                return [current_text]
            
            # å¦‚æœæ²¡æœ‰æ›´å¤šåˆ†éš”ç¬¦ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²
            if not current_separators:
                return self._split_by_length(current_text)
            
            # è·å–å½“å‰åˆ†éš”ç¬¦
            separator = current_separators[0]
            remaining_separators = current_separators[1:]
            
            # ä½¿ç”¨å½“å‰åˆ†éš”ç¬¦åˆ†å‰²
            if separator:
                parts = current_text.split(separator)
            else:
                # ç©ºå­—ç¬¦ä¸²åˆ†éš”ç¬¦è¡¨ç¤ºæŒ‰å­—ç¬¦
                return self._split_by_length(current_text)
            
            # åˆå¹¶å°ç‰‡æ®µ
            merged_parts = []
            current_part = ""
            
            for part in parts:
                # å¦‚æœå½“å‰éƒ¨åˆ†ä¸ºç©ºï¼Œç›´æ¥æ·»åŠ åˆ†éš”ç¬¦
                if not current_part:
                    current_part = part + (separator if separator != "" else "")
                # å¦‚æœæ·»åŠ æ–°éƒ¨åˆ†åä»å°äºå—å¤§å°ï¼Œåˆå¹¶
                elif len(current_part) + len(separator) + len(part) <= self.chunk_size:
                    current_part += separator + part
                # å¦åˆ™ï¼Œä¿å­˜å½“å‰éƒ¨åˆ†ï¼Œå¼€å§‹æ–°çš„éƒ¨åˆ†
                else:
                    if current_part:
                        merged_parts.append(current_part)
                    current_part = part
            
            # æ·»åŠ æœ€åçš„éƒ¨åˆ†
            if current_part:
                merged_parts.append(current_part)
            
            # å¦‚æœåˆ†å‰²ç»“æœåªæœ‰1ä¸ªï¼Œå°è¯•ä¸‹ä¸€ä¸ªåˆ†éš”ç¬¦
            if len(merged_parts) == 1:
                return recursive_split(current_text, remaining_separators)
            
            # é€’å½’å¤„ç†æ¯ä¸ªéƒ¨åˆ†
            final_chunks = []
            for part in merged_parts:
                chunks = recursive_split(part, self.separators)
                final_chunks.extend(chunks)
            
            return final_chunks
        
        # å¼€å§‹é€’å½’åˆ†å‰²
        chunks = recursive_split(text, self.separators)
        
        # åº”ç”¨é‡å 
        if self.chunk_overlap > 0 and len(chunks) > 1:
            chunks = self._apply_overlap(chunks)
        
        return chunks
    
    def _split_by_length(self, text: str) -> List[str]:
        """æŒ‰å›ºå®šé•¿åº¦åˆ†å‰²"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            chunks.append(text[start:end])
            start = end
        
        return chunks
    
    def _apply_overlap(self, chunks: List[str]) -> List[str]:
        """åº”ç”¨é‡å """
        if len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = []
        
        for i in range(len(chunks)):
            current_chunk = chunks[i]
            
            # æ·»åŠ ä¸Šä¸€ä¸ªå—çš„é‡å éƒ¨åˆ†
            if i > 0 and self.chunk_overlap > 0:
                prev_chunk = chunks[i-1]
                overlap_start = max(0, len(prev_chunk) - self.chunk_overlap)
                overlap_text = prev_chunk[overlap_start:]
                current_chunk = overlap_text + current_chunk
            
            # æ·»åŠ ä¸‹ä¸€ä¸ªå—çš„é‡å éƒ¨åˆ†
            if i < len(chunks) - 1 and self.chunk_overlap > 0:
                next_chunk = chunks[i+1]
                overlap_text = next_chunk[:min(self.chunk_overlap, len(next_chunk))]
                current_chunk = current_chunk + overlap_text
            
            overlapped_chunks.append(current_chunk)
        
        return overlapped_chunks
    
    def split_by_sentences(self, text: str) -> List[str]:
        """
        æŒ‰å¥å­åˆ†å‰²ï¼ˆé€‚åˆä¸­è‹±æ–‡æ··åˆæ–‡æœ¬ï¼‰
        """
        # ä¸­è‹±æ–‡å¥å­åˆ†å‰²æ­£åˆ™è¡¨è¾¾å¼
        sentence_pattern = r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+'
        sentences = re.split(sentence_pattern, text)
        
        # åˆå¹¶çŸ­å¥å­
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += " " + sentence if current_chunk else sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks


if __name__ == "__main__":
    spliter = SimpleTextSplitter()
    # rager = RealTimeRAG()
    query = "deepfake"
    docs = ["AI-Generated or Real? Please identify whether the image is real or fake and how confident you are. Scroll Down. â†“ Advice: ğŸ§â€â™‚ï¸ Notice the posture and overall appearance of the e peopleâ€”do they look consistent and realistic? ğŸšª Check the cabinets and objectsâ€”do the door handles make sense and appear properly placed? Real: This is a real image. Fake: This is an AI-generated image. How confident are you? Not at all Slightly Moderately Very Perfectly Submit your initial guess Real or AI-Generated? Real: This is a real image. Fake: This is an AI-generated image. How confident are you? Not at all Slightly Moderately Very Perfectly How much do you think others would agree with your judgment? Almost no one Few About half Most Almost everyone Submit your final guess Real: This is a real image Next Image", 
            "ä¸€æ–‡é€Ÿè§ˆæ·±åº¦ä¼ªé€ æ£€æµ‹ï¼ˆDetection of Deepfakesï¼‰ï¼šæœªæ¥æŠ€æœ¯çš„å®ˆé—¨äºº å‰è¨€ ä¸€ã€DeepfakesæŠ€æœ¯åŸç† å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼šç»†è‡´çš„è‰ºæœ¯å­¦å¾’ ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼šç”»å®¶ä¸è¯„å®¡çš„åŒé‡è§’è‰² è®­ç»ƒè¿‡ç¨‹ï¼šæŠ€è‰ºçš„ç£¨ç»ƒ åº”ç”¨å’ŒæŒ‘æˆ˜ äºŒã€Detection of DeepfakesæŠ€æœ¯åŸç†ï¼šè§£å¯†æ•°å­—ä¼ªè£… ç‰¹å¾æå–ï¼šå¯»æ‰¾æ•°å­—è¶³è¿¹ å¼‚å¸¸æ£€æµ‹ï¼šå¯»æ‰¾ä¸å’Œè°çš„æ—‹å¾‹ æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼šæ„å»ºæ™ºèƒ½çš„å®ˆé—¨äºº å¤šæ¨¡æ€åˆ†æï¼šå…¨æ–¹ä½çš„ç›‘æ§ç³»ç»Ÿ æœªæ¥å±•æœ›ï¼šæŒ‘æˆ˜ä¸æœºé‡å¹¶å­˜ ğŸŒˆä½ å¥½å‘€ï¼æˆ‘æ˜¯ æ˜¯Yuæ¬¸ ğŸŒŒ 2024æ¯æ—¥ç™¾å­—ç¯†åˆ»æ—¶å…‰ï¼Œæ„Ÿè°¢ä½ çš„é™ªä¼´ä¸æ”¯æŒ ~ ğŸš€ æ¬¢è¿ä¸€èµ·è¸ä¸Šæ¢é™©ä¹‹æ—…ï¼ŒæŒ–æ˜æ— é™å¯èƒ½ï¼Œå…±åŒæˆé•¿ï¼ å‰äº›å¤©å‘ç°äº†ä¸€ä¸ªäººå·¥æ™ºèƒ½å­¦ä¹ ç½‘ç«™ï¼Œå†…å®¹æ·±å…¥æµ…å‡ºã€æ˜“äºç†è§£ã€‚å¦‚æœå¯¹äººå·¥æ™ºèƒ½æ„Ÿå…´è¶£ï¼Œä¸å¦¨ ç‚¹å‡»æŸ¥çœ‹ ã€‚ å‰è¨€ åœ¨æ•°å­—åŒ–æ—¶ä»£çš„é«˜é€Ÿå…¬è·¯ä¸Šï¼Œæ·±åº¦ä¼ªé€ æŠ€æœ¯ï¼ˆDeepfakeï¼‰å¦‚åŒä¸€è¾†æ— äººé©¾é©¶çš„è·‘è½¦ï¼Œå…¶é€Ÿåº¦æƒŠäººï¼Œæ½œåŠ›å·¨å¤§ï¼ŒåŒæ—¶ä¹Ÿå¸¦æ¥äº†æ½œåœ¨çš„å±é™©ã€‚ æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼ˆDetection of Deepfakesï¼‰ä¸ä»…æ˜¯ä¸€åœºç§‘æŠ€ç•Œçš„å†›å¤‡ç«èµ›ï¼Œæ›´æ˜¯æœªæ¥æ•°å­—å†…å®¹å®‰å…¨é¢†åŸŸçš„é»„é‡‘çŸ¿è„‰ã€‚æœ¬æ–‡å°†æ¢è®¨è¿™ä¸€æŠ€æœ¯çš„æ ¸å¿ƒåŸç†ï¼Œæ­ç¤ºå…¶å¦‚ä½•æˆä¸ºæ•°å­—æ—¶ ä»£å®ˆé—¨äººçš„è§’è‰²ã€‚ é‡ç°å’Œæ›¿æ¢çš„å¯¹æ¯” ç¼–è¾‘ åˆæˆï¼š å‚è€ƒï¼šhttps://zhuanlan.zhihu.com/p/139489768 https://zhuanlan.zhihu.com/p/564661269 ä¸€ã€DeepfakesæŠ€æœ¯åŸç† DeepfakesæŠ€æœ¯ï¼Œæ˜¯ä¸€ç§åŸº äºæ·±åº¦å­¦ä¹ çš„å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘åˆæˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿåˆ›å»ºçœ‹èµ·æ¥éå¸¸çœŸå®çš„å‡è±¡ã€‚è¿™é¡¹æŠ€æœ¯çš„åå­—æ¥æºäºâ€œæ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰â€å’Œâ€œå‡å†’ï¼ˆFakeï¼‰â€çš„ç»“åˆï¼Œå®ƒåˆ©ç”¨äº†æ·±åº¦å­¦ä¹ çš„ä¸€ç§ç‰¹æ®Šå½¢å¼â€”â€”å·ç§¯ ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¥å®ç°å…¶æ ¸å¿ƒåŠŸèƒ½ã€‚ å°†DeepfakesæŠ€æœ¯æ¯”å–»ä¸ºä¸€ä½é«˜è¶…çš„ç”»å®¶å’Œä»–çš„æŒ‘å‰”è¯„å®¡ï¼Œå¯ä»¥å½¢è±¡åœ°è§£é‡Šè¿™é¡¹æŠ€æœ¯èƒŒåçš„ä¸“ä¸šæœ¯è¯­å’ŒåŸç†ã€‚åœ¨è¿™ä¸ªæ¯”å–»ä¸­ï¼Œæ·±åº¦å­¦ä¹ çš„å¤æ‚ä¸–ç•Œè¢«ç®€åŒ–ä¸ºè‰ºæœ¯åˆ›ä½œçš„è¿‡ç¨‹ï¼Œæ—¨åœ¨åˆ›é€ å‡ºè¶³ä»¥æ¬ºéª—è§‚ä¼—çœ¼ç›çš„ä½œå“ã€‚ ä»¥ä¸‹æ˜¯DeepfakesæŠ€æœ¯åŸç†çš„ç®€è¦ä»‹ç»ï¼š å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼šç»†è‡´çš„è‰ºæœ¯å­¦å¾’ CNNæ˜¯ä¸€ç±»ç‰¹åˆ«è®¾è®¡æ¥è¯†åˆ«å’Œå¤„ç†å›¾åƒçš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚åœ¨DeepfakesæŠ€æœ¯ä¸­ï¼ŒCNNç”¨äºåˆ†æå’Œç†è§£è¾“å…¥çš„å›¾åƒæˆ–è§†é¢‘å¸§ï¼Œå¦‚äººè„¸çš„ç‰¹å¾å’Œè¡¨æƒ…ã€‚CNNé€šè¿‡ä»å¤§é‡çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œèƒ½å¤Ÿè¯†åˆ«ä¸åŒäººè„¸çš„ç»†å¾®å·®å¼‚ï¼Œå¹¶æå–å‡ºå…³é”®ç‰¹å¾ï¼Œä¸ºåç»­çš„å¤„ç†æ­¥éª¤ æ‰“ä¸‹åŸºç¡€ã€‚ æƒ³è±¡ä¸€ä½å¹´è½»çš„è‰ºæœ¯å­¦å¾’ï¼ˆCNNï¼‰ï¼Œä»–æ­£åœ¨å­¦ä¹ å¦‚ä½•ç²¾ç¡®åœ°æ•æ‰äººç‰©çš„é¢éƒ¨ç‰¹å¾å’Œè¡¨æƒ…ã€‚é€šè¿‡è§‚å¯Ÿæˆåƒä¸Šä¸‡çš„è‚–åƒç”»ï¼Œè¿™ä½å­¦å¾’å­¦ä¼šäº†å¦‚ä½•è¯†åˆ«é¢éƒ¨çš„æ¯ä¸€æ¡çº¿æ¡å’Œé˜´å½±ï¼Œå°±åƒCNNé€šè¿‡åˆ†æ å¤§é‡å›¾åƒæ•°æ®å­¦ä¹ è¯†åˆ«å’Œå¤„ç†å›¾åƒç‰¹å¾ä¸€æ ·ã€‚å­¦å¾’çš„ç›®æ ‡æ˜¯æŒæ¡å¤åˆ¶ä»»ä½•äººç‰©é¢éƒ¨ç‰¹å¾çš„æŠ€è‰ºï¼Œä»¥è‡³äºä»–çš„ä½œå“å¯ä»¥ä¸åŸä½œåª²ç¾ã€‚ ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼šç”»å®¶ä¸è¯„å®¡çš„åŒé‡è§’è‰² GANæ˜¯ç”±ä¸¤éƒ¨åˆ†ç»„æˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼šä¸€ä¸ªç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰å’Œä¸€ä¸ªé‰´åˆ«å™¨ï¼ˆDiscriminatorï¼‰ã€‚åœ¨Deepfakesä¸­ï¼Œç”Ÿæˆå™¨çš„ä»»åŠ¡æ˜¯åˆ›å»ºå°½å¯èƒ½çœŸå®çš„å‡å›¾åƒæˆ–è§†é¢‘å¸§ï¼Œè€Œé‰´åˆ«å™¨çš„ä»»åŠ¡åˆ™æ˜¯åŒºåˆ†ç”Ÿæˆçš„å›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´ çš„å·®å¼‚ã€‚è¿™ä¸¤ä¸ªç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’ç«äº‰ï¼Œç”Ÿæˆå™¨ä¸æ–­å­¦ä¹ å¦‚ä½•æ”¹è¿›å…¶ç”Ÿæˆçš„å›¾åƒï¼Œä»¥ä½¿å…¶æ›´éš¾è¢«é‰´åˆ«å™¨è¯†åˆ«ï¼Œè€Œé‰´åˆ«å™¨åˆ™ä¸æ–­æé«˜å…¶è¯†åˆ«çœŸä¼ªçš„èƒ½åŠ›ã€‚è¿™ä¸ªè¿‡ç¨‹æœ€ç»ˆä¼šå¯¼è‡´ç”Ÿæˆçš„å›¾åƒè´¨é‡æ˜¾è‘—æé«˜ï¼Œè¶³ä»¥ä»¥å‡ä¹±çœŸã€‚ åœ¨è¿™ä¸ªè‰ºæœ¯ä¸–ç•Œé‡Œï¼Œæœ‰ä¸€ä½å¤©æ‰ç”»å®¶ï¼ˆç”Ÿæˆå™¨ï¼‰å’Œä¸€ä½æå…¶æŒ‘å‰”çš„è‰ºæœ¯è¯„è®ºå®¶ï¼ˆé‰´åˆ«å™¨ï¼‰ä¸æ–­åœ°è¾ƒé‡ã€‚ç”»å®¶çš„ç›®æ ‡æ˜¯åˆ›ä½œå‡ºæå…¶é€¼çœŸçš„è‚–åƒç”»ï¼Œä»¥è‡³äºè¿æœ€ç»†å¾®çš„ç»†èŠ‚éƒ½èƒ½æ¬ºéª—è§‚ä¼— ã€‚æ¯æ¬¡ç”»å®¶å®Œæˆä¸€å¹…ä½œå“æ—¶ï¼Œè¯„è®ºå®¶éƒ½ä¼šä»”ç»†å®¡æŸ¥ï¼Œè¯•å›¾æ‰¾å‡ºä»»ä½•å¯èƒ½æ­ç¤ºä½œå“ä¸ºå¤åˆ¶å“çš„çº¿ç´¢ã€‚å¦‚æœè¯„è®ºå®¶æŒ‡å‡ºäº†ä½œå“çš„ç‘•ç–µï¼Œç”»å®¶å°±ä¼šæ ¹æ®è¿™äº›åé¦ˆå›å»ä¿®æ­£ï¼Œæ¯æ¬¡éƒ½è¯•å›¾åˆ›ä½œå‡ºæ›´åŠ å®Œç¾çš„ä½œå“ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸æ–­é‡å¤ï¼Œç”»å®¶çš„æŠ€è‰ºï¼ˆç”Ÿæˆå™¨çš„ç”Ÿæˆèƒ½åŠ›ï¼‰å’Œè¯„è®ºå®¶çš„é‰´èµçœ¼å…‰ï¼ˆé‰´åˆ«å™¨çš„è¾¨åˆ«èƒ½åŠ›ï¼‰éƒ½åœ¨ä¸æ–­æé«˜ã€‚ è®­ç»ƒè¿‡ç¨‹ï¼šæŠ€è‰ºçš„ç£¨ç»ƒ åœ¨åˆ›å»ºDeepfakesæ—¶ï¼Œé¦–å…ˆéœ€è¦æ”¶é›†å¤§é‡çš„ç›®æ ‡äººç‰©çš„å›¾åƒæˆ– è§†é¢‘èµ„æ–™ï¼Œä½œä¸ºè®­ç»ƒæ•°æ®ã€‚è¿™äº›æ•°æ®è¢«ç”¨æ¥è®­ç»ƒGANï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆå™¨ï¼Œä»¥å­¦ä¹ å¦‚ä½•äº§ç”Ÿç›®æ ‡äººç‰©çš„å‡†ç¡®å’ŒçœŸå®çš„é¢éƒ¨ç‰¹å¾ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆå™¨å°è¯•åˆ›å»ºè¶Šæ¥è¶ŠçœŸå®çš„å›¾åƒï¼Œè€Œé‰´åˆ«å™¨åˆ™å°è¯•å‡†ç¡®åœ°åŒºåˆ† çœŸå®å›¾åƒå’Œç”Ÿæˆå›¾åƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹é€æ¸å­¦ä¼šç”Ÿæˆé«˜è´¨é‡çš„å‡å›¾åƒæˆ–è§†é¢‘ã€‚ åœ¨DeepfakesæŠ€æœ¯çš„èƒŒåï¼Œè¿™åœºè‰ºæœ¯çš„è¾ƒé‡å®é™…ä¸Šæ˜¯ä¸€ä¸ªå¤æ‚çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…¶ä¸­åŒ…æ‹¬äº†æ— æ•°æ¬¡çš„å°è¯•å’Œé”™è¯¯ï¼Œç”»å®¶ï¼ˆç”Ÿæˆå™¨ï¼‰ä¸æ–­å°è¯•åˆ›ä½œå‡ºæ–°çš„ä½œå“ï¼Œè€Œè¯„è®ºå®¶ï¼ˆé‰´åˆ«å™¨ï¼‰åˆ™æŒç»­æä¾›å…³é”®çš„åé¦ˆã€‚è¿™ä¸ªè¿‡ç¨‹éœ€è¦å¤§é‡çš„â€œè‰ºæœ¯ä½œå“â€ï¼ˆå›¾åƒæ•°æ®ï¼‰ä½œä¸ºè®­ç»ƒææ–™ï¼Œä»¥ç¡®ä¿ç”»å®¶èƒ½å¤Ÿå­¦ä¹ åˆ°åˆ¶ä½œå„ç§ä¸åŒé£æ ¼å’Œè¡¨æƒ…çš„æŠ€å·§ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œç”»å®¶å˜å¾—è¶³å¤Ÿç†Ÿç»ƒï¼Œä»¥è‡³äºä»–çš„ä½œå“å¯ä»¥è½»æ˜“åœ°ä¸çœŸå®çš„è‚–åƒç”»æ··æ·†ã€‚ åº”ç”¨å’ŒæŒ‘æˆ˜ DeepfakesæŠ€æœ¯çš„å‘å±•ï¼Œè™½ç„¶åœ¨å¨±ä¹ã€ç”µå½±åˆ¶ä½œã€ä¸ªäººéšç§ä¿æŠ¤ç­‰é¢†åŸŸæä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œä½†åŒæ—¶ä¹Ÿ å¼•å‘äº†ä¼¦ç†ã€æ³•å¾‹å’Œç¤¾ä¼šå®‰å…¨æ–¹é¢çš„é‡å¤§å…³åˆ‡ã€‚å› ä¸ºå®ƒå¯ä»¥è¢«ç”¨æ¥åˆ¶ä½œè¯¯å¯¼æ€§çš„å†…å®¹ï¼Œå½±å“å…¬ä¼—èˆ†è®ºï¼Œç”šè‡³æŸå®³ä¸ªäººå£°èª‰ã€‚ è™½ç„¶è¿™ä½ç”»å®¶ï¼ˆDeepfakesç”Ÿæˆå™¨ï¼‰çš„æŠ€è‰ºä»¤äººé’¦ä½©ï¼Œä½†ä»–çš„èƒ½åŠ›ä¹Ÿå¼•å‘äº†ä¸€ç³»åˆ—ä¼¦ç†å’Œé“å¾·ä¸Šçš„é—®é¢˜ã€‚åœ¨è¿™ä¸ªæ•°å­—åŒ–çš„è‰ºæœ¯ä¸–ç•Œä¸­ï¼Œä»–çš„ä½œå“å¯èƒ½è¢«ç”¨äºåˆ›é€ è¯¯å¯¼æ€§çš„å†…å®¹ï¼Œå½±å“å…¬ä¼—æ„è§æˆ–æŸå®³ä¸ªäººå£°èª‰ã€‚å› æ­¤ï¼Œè™½ç„¶è¿™é¡¹æŠ€æœ¯å±•ç¤ºäº†æ·±åº¦å­¦ä¹ çš„å·¨å¤§æ½œåŠ›ï¼Œä½†åŒæ—¶ä¹Ÿæé†’æˆ‘ä»¬éœ€è¦è°¨æ…åœ°è€ƒè™‘å…¶åº”ç”¨çš„ç•Œé™å’Œåæœã€‚ æ€»ä¹‹ï¼ŒDeepfakesæŠ€æœ¯çš„åŸç†æ¶‰åŠåˆ°å¤æ‚çš„æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œå°¤å…¶æ˜¯CNNå’ŒGANï¼Œå®ƒä»¬å…±åŒä½œç”¨äºç”Ÿæˆéš¾ä»¥åŒºåˆ†çœŸä¼ªçš„å›¾åƒå’Œè§†é¢‘ã€‚éšç€æŠ€æœ¯çš„å‘å±•ï¼Œå¦‚ä½•å¹³è¡¡å…¶åˆ›æ–°åº”ç”¨ä¸æ½œåœ¨é£é™©ï¼Œæˆä¸ºäº†ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚ äºŒã€Detection of DeepfakesæŠ€æœ¯åŸç†ï¼šè§£å¯†æ•°å­—ä¼ªè£… ç‰¹å¾æå–ï¼šå¯»æ‰¾æ•°å­—è¶³è¿¹ æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœå°†æ¯ä¸ªè§†é¢‘æ¯”ä½œä¸€ä¸ªå¤æ‚çš„è¿·å®«ï¼Œé‚£ä¹ˆæ·±åº¦ä¼ªé€ è§†é¢‘æ£€ æµ‹æŠ€æœ¯å°±æ˜¯é‚£äº›è¯•å›¾æ‰¾åˆ°å‡ºå£çš„æ¢é™©è€…ã€‚è¿™äº›æ¢é™©è€…ï¼ˆæ£€æµ‹ç®—æ³•ï¼‰é¦–å…ˆéœ€è¦è¯†åˆ«è¿·å®«ä¸­çš„å…³é”®çº¿ç´¢ï¼ˆè§†é¢‘ç‰¹å¾ï¼‰ï¼Œè¿™åŒ…æ‹¬äº†é¢éƒ¨çš„å¾®å¦™å˜åŒ–ã€çœ¼ç›çš„é—ªçƒé¢‘ç‡ï¼Œç”šè‡³æ˜¯å…‰çº¿æŠ•å°„çš„æ–¹å¼ã€‚é€šè¿‡ç²¾ç¡®åˆ†æè¿™äº›ç»†å¾®çš„çº¿ç´¢ï¼Œæ£€æµ‹ç®—æ³•å¯ä»¥å¼€å§‹åˆ¤æ–­è¿™ä¸ªè¿·å®«æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œè¿˜æ˜¯æŸç§æŠ€æœ¯åˆ›é€ å‡ºæ¥çš„å¹»è±¡ã€‚ å¼‚å¸¸æ£€æµ‹ï¼šå¯»æ‰¾ä¸å’Œè°çš„æ—‹å¾‹ å°†æ¯ä¸ªè§†é¢‘æ¯”ä½œä¸€é¦–æ›²å­ï¼Œé‚£ä¹ˆå¼‚å¸¸æ£€æµ‹å°±åœ¨äºè¾¨è¯†å‡ºå…¶ä¸­çš„ä¸å’Œè°éŸ³ç¬¦ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹é€šè¿‡å¤§é‡çš„è®­ç»ƒï¼Œå­¦ä¼šäº†è¯†åˆ«å“ªäº›éŸ³ç¬¦ï¼ˆè§†é¢‘ç‰¹å¾ï¼‰å±äºæ­£å¸¸çš„æ—‹å¾‹ï¼Œå“ªäº›åˆ™æš—ç¤ºç€æ›²å­è¢«äººä¸ºç¯¡æ”¹ã€‚è¿™å°±åƒä¸€ä½ç»éªŒä¸°å¯Œçš„éŸ³ä¹å®¶èƒ½å¤Ÿå‡­å€Ÿç»†è…»çš„å¬è§‰å¯Ÿè§‰å‡ºæ¼”å¥ä¸­çš„å¾®å°å¤±è¯¯ã€‚ æ·± åº¦å­¦ä¹ æ¨¡å‹ï¼šæ„å»ºæ™ºèƒ½çš„å®ˆé—¨äºº æ·±åº¦å­¦ä¹ æ¨¡å‹æ˜¯æ·±åº¦è™šå‡è§†é¢‘æ£€æµ‹æŠ€æœ¯çš„æ ¸å¿ƒï¼Œå®ƒä»¬å°±åƒæ˜¯è®­ç»ƒæœ‰ç´ çš„å®ˆé—¨äººï¼Œå®ˆæŠ¤ç€æ•°å­—å†…å®¹çš„çœŸå®æ€§ã€‚é€šè¿‡å¯¹å¤§é‡çœŸå®å’Œä¼ªé€ è§†é¢‘çš„å­¦ä¹ ï¼Œè¿™äº›å®ˆé—¨äººé€æ¸æŒæ¡äº† åŒºåˆ†äºŒè€…çš„èƒ½åŠ›ã€‚æ— è®ºä¼ªé€ æŠ€æœ¯å¦‚ä½•è¿›æ­¥ï¼Œåªè¦æŒç»­å¯¹è¿™äº›å®ˆé—¨äººè¿›è¡Œè®­ç»ƒï¼Œå®ƒä»¬å°±èƒ½é€‚åº”æ–°çš„æŒ‘æˆ˜ï¼Œä¿æŠ¤æ•°å­—ä¸–ç•Œçš„å®‰å…¨ã€‚ å¤šæ¨¡æ€åˆ†æï¼šå…¨æ–¹ä½çš„ç›‘æ§ç³»ç»Ÿ åœ¨æ·±åº¦è™šå‡è§†é¢‘æ£€æµ‹ä¸­ï¼Œä»…ä»…åˆ†æè§†é¢‘æ˜¯ä¸å¤Ÿçš„ï¼Œå°±åƒä¸€åº§è¦å¡ä¸å¯èƒ½åªä¾é ä¸€é“é˜²çº¿ã€‚å¤šæ¨¡æ€åˆ†æå…è®¸æ£€æµ‹ç³»ç»ŸåŒæ—¶ç›‘æ§è§†é¢‘å’ŒéŸ³é¢‘ï¼Œç”šè‡³æ˜¯å®ƒä»¬ä¹‹é—´çš„å…³è”ï¼Œä»è€Œæ„å»ºèµ·ä¸€å¥—æ›´ä¸ºå…¨é¢çš„é˜²å¾¡æœºåˆ¶ã€‚è¿™å°±åƒæ˜¯åœ¨è¦å¡çš„æ¯ä¸ªè§’è½éƒ½éƒ¨ç½²äº†å“¨å…µï¼Œæ— è®ºæ•Œäººä»å“ªä¸ªæ–¹å‘æ¥è¢­ï¼Œéƒ½èƒ½è¢«åŠæ—¶å‘ç°å’Œæ‹¦æˆªã€‚ æœªæ¥å±•æœ›ï¼šæŒ‘æˆ˜ä¸æœºé‡å¹¶å­˜ éšç€æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„ä¸æ–­è¿›åŒ–ï¼Œæ·±åº¦è™šå‡è§†é¢‘æ£€æµ‹é¢ä¸´ç€å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œæ­£æ˜¯è¿™ç§æŒ‘æˆ˜ï¼Œæä¾›äº†ç‹¬ç‰¹çš„æœºé‡ã€‚ è¿™ä¸€é¢†åŸŸçš„å…ˆè¿›æŠ€æœ¯å’Œè§£å†³æ–¹æ¡ˆï¼Œä¸ä»…å¯ä»¥ä¿æŠ¤ç¤¾ä¼šå…å—è™šå‡ä¿¡æ¯çš„ä¾µå®³ï¼Œä¹Ÿèƒ½åœ¨æœªæ¥çš„æ•°å­—å®‰å…¨é¢†åŸŸå æ®æœ‰åˆ©åœ°ä½ã€‚ ä½œä¸ºæœªæ¥æŠ€æœ¯çš„å®ˆé—¨äººï¼Œæ·±åº¦è™šå‡è§†é¢‘æ£€æµ‹æŠ€æœ¯æ­£ç«™åœ¨é£å£æµªå°–ï¼Œå…±åŒå®ˆæŠ¤æ•°å­—ä¸–ç•Œçš„çœŸå®æ€§å’Œå®‰å…¨æ€§ã€‚"]
    
    chunks_list = []
    for doc in docs:
        chunks = spliter.split_text(doc)
        chunks_list += chunks
    print(len(chunks_list))
    # top_k = rager.execute(query, chunks_list)
    # print(top_k)