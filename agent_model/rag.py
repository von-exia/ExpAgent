import os
import glob
import time
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union

from sentence_transformers import SentenceTransformer


class LocalEmbeddingModel:
    def __init__(self, model_name):
        start_time = time.time()
        self.embedding_model = SentenceTransformer(model_name)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        end_time = time.time()
        print(f"âœ… Embedding model loaded in {end_time - start_time:.2f}s")
        print(f"ğŸ“Š Embedding dimension: {self.embedding_dim}")
        
    def encode(self, query, documents):
        query_embeddings = self.embedding_model.encode(query, prompt_name="query", normalize_embeddings=True)
        document_embeddings = self.embedding_model.encode(documents, normalize_embeddings=True)
        return query_embeddings, document_embeddings
            

class RealTimeRAG:
    def __init__(self, 
                 embedding_model_name: str = "./models/Qwen3-0.6B-embedding",
                 embedding_model = None,
                 chunk_size: int = 500,
                 chunk_overlap: int = 50
                 ):
        """
        Args:
            embedding_model_name: SentenceTransformeræ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å‰²å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å‰²é‡å å¤§å°
            persist_directory: FAISSç´¢å¼•å­˜å‚¨æ ¹ç›®å½•
        """
        self.embedding_model_name = embedding_model_name
        self.embedding_model = embedding_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        self._initialize_components()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        print("ğŸ”§ Initializing RAG components...")
        if self.embedding_model is None:
            print(f"ğŸ”¢ Loading Local embedding model: {self.embedding_model_name}")
            self.embedding_model = LocalEmbeddingModel(self.embedding_model_name)
        self.text_spliter = SimpleTextSplitter(self.chunk_size, self.chunk_overlap)
        print("âœ… Real-time RAG components initialized successfully!")
    
    
    def execute(self, 
                query: str, 
                documents: List[str],
                k: int = 5,
                return_scores: bool = False,
                score_threshold: Optional[float] = None) -> Union[List[str], Tuple[List[str], List[float]]]:
        """
        æ‰§è¡Œæ£€ç´¢ï¼Œè¿”å›ä¸queryæœ€ç›¸ä¼¼çš„å‰kä¸ªæ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            return_scores: æ˜¯å¦è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
            score_threshold: åˆ†æ•°é˜ˆå€¼ï¼Œåªè¿”å›åˆ†æ•°é«˜äºè¯¥å€¼çš„æ–‡æ¡£
            
        Returns:
            å¦‚æœ return_scores=True: è¿”å› (documents, scores)
            å¦‚æœ return_scores=False: è¿”å› documents
        """
        if not documents:
            return ([], []) if return_scores else []
        
        if isinstance(query, str):
            query = [query]
            
        if self.text_spliter:
            chunks_list = []
            for doc in documents:
                chunks = self.text_spliter.split_text(doc)
                chunks_list += chunks
            documents = chunks_list[:10] # only use top chunk for debug and for the limitation of API
                    
        # Encode query and documents
        query_embeddings, document_embeddings = self.embedding_model.encode(query, documents)
        
        # Compute cosine similarity
        # similarity_matrix's shape is (1, num_documents)
        similarity_matrix = np.matmul(query_embeddings, document_embeddings.T)
        similarity_scores = similarity_matrix[0]
        
        # Sort by scores
        if k > len(documents):
            k = len(documents)
        top_k_indices = np.argsort(similarity_scores)[-k:][::-1]
        
        # Apply score threshold (if provided)
        if score_threshold is not None:
            valid_indices = [idx for idx in top_k_indices if similarity_scores[idx] >= score_threshold]
            if not valid_indices:
                return ([], []) if return_scores else []
            top_k_indices = np.array(valid_indices)
        
        # Get corresponding docs and scores
        top_documents = [documents[idx] for idx in top_k_indices]
        top_scores = [float(similarity_scores[idx]) for idx in top_k_indices]
        
        if return_scores:
            return top_documents, top_scores
        else:
            return top_documents
        
    
import re
from typing import List


class SimpleTextSplitter:
    """
    è½»é‡çº§æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆæ— é¢å¤–ä¾èµ–ï¼‰
    """
    
    def __init__(self, 
                 chunk_size: int = 500,
                 chunk_overlap: int = 50,
                 separators: List[str] = None):
        """
        Args:
            chunk_size: å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: é‡å å¤§å°
            separators: åˆ†éš”ç¬¦åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        if separators is None:
            # ä¸­æ–‡å‹å¥½çš„åˆ†éš”ç¬¦
            self.separators = [
                # "\n\n",      # åŒæ¢è¡Œï¼ˆæ®µè½ï¼‰
                # "\n",        # å•æ¢è¡Œ
                # "ã€‚",        # å¥å·
                # "ï¼",        # æ„Ÿå¹å·
                # "ï¼Ÿ",        # é—®å·
                # "ï¼›",        # åˆ†å·
                # "ï¼Œ",        # é€—å·
                # " ",         # ç©ºæ ¼
                # ""           # æœ€åæŒ‰å­—ç¬¦åˆ†å‰²
            ]
        else:
            self.separators = separators
    
    def split_text(self, text: str) -> List[str]:
        """
        åˆ†å‰²æ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text:
            return []
        
        # é€’å½’åˆ†å‰²å‡½æ•°
        def recursive_split(current_text: str, current_separators: List[str]) -> List[str]:
            # å¦‚æœæ–‡æœ¬å·²ç»è¶³å¤Ÿå°ï¼Œç›´æ¥è¿”å›
            if len(current_text) <= self.chunk_size:
                return [current_text]
            
            # å¦‚æœæ²¡æœ‰æ›´å¤šåˆ†éš”ç¬¦ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²
            if not current_separators:
                return self._split_by_length(current_text)
            
            # è·å–å½“å‰åˆ†éš”ç¬¦
            separator = current_separators[0]
            remaining_separators = current_separators[1:]
            
            # ä½¿ç”¨å½“å‰åˆ†éš”ç¬¦åˆ†å‰²
            if separator:
                parts = current_text.split(separator)
            else:
                # ç©ºå­—ç¬¦ä¸²åˆ†éš”ç¬¦è¡¨ç¤ºæŒ‰å­—ç¬¦
                return self._split_by_length(current_text)
            
            # åˆå¹¶å°ç‰‡æ®µ
            merged_parts = []
            current_part = ""
            
            for part in parts:
                # å¦‚æœå½“å‰éƒ¨åˆ†ä¸ºç©ºï¼Œç›´æ¥æ·»åŠ åˆ†éš”ç¬¦
                if not current_part:
                    current_part = part + (separator if separator != "" else "")
                # å¦‚æœæ·»åŠ æ–°éƒ¨åˆ†åä»å°äºå—å¤§å°ï¼Œåˆå¹¶
                elif len(current_part) + len(separator) + len(part) <= self.chunk_size:
                    current_part += separator + part
                # å¦åˆ™ï¼Œä¿å­˜å½“å‰éƒ¨åˆ†ï¼Œå¼€å§‹æ–°çš„éƒ¨åˆ†
                else:
                    if current_part:
                        merged_parts.append(current_part)
                    current_part = part
            
            # æ·»åŠ æœ€åçš„éƒ¨åˆ†
            if current_part:
                merged_parts.append(current_part)
            
            # å¦‚æœåˆ†å‰²ç»“æœåªæœ‰1ä¸ªï¼Œå°è¯•ä¸‹ä¸€ä¸ªåˆ†éš”ç¬¦
            if len(merged_parts) == 1:
                return recursive_split(current_text, remaining_separators)
            
            # é€’å½’å¤„ç†æ¯ä¸ªéƒ¨åˆ†
            final_chunks = []
            for part in merged_parts:
                chunks = recursive_split(part, self.separators)
                final_chunks.extend(chunks)
            
            return final_chunks
        
        # å¼€å§‹é€’å½’åˆ†å‰²
        chunks = recursive_split(text, self.separators)
        
        # åº”ç”¨é‡å 
        if self.chunk_overlap > 0 and len(chunks) > 1:
            chunks = self._apply_overlap(chunks)
        
        return chunks
    
    def _split_by_length(self, text: str) -> List[str]:
        """æŒ‰å›ºå®šé•¿åº¦åˆ†å‰²"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            chunks.append(text[start:end])
            start = end
        
        return chunks
    
    def _apply_overlap(self, chunks: List[str]) -> List[str]:
        """åº”ç”¨é‡å """
        if len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = []
        
        for i in range(len(chunks)):
            current_chunk = chunks[i]
            
            # æ·»åŠ ä¸Šä¸€ä¸ªå—çš„é‡å éƒ¨åˆ†
            if i > 0 and self.chunk_overlap > 0:
                prev_chunk = chunks[i-1]
                overlap_start = max(0, len(prev_chunk) - self.chunk_overlap)
                overlap_text = prev_chunk[overlap_start:]
                current_chunk = overlap_text + current_chunk
            
            # æ·»åŠ ä¸‹ä¸€ä¸ªå—çš„é‡å éƒ¨åˆ†
            if i < len(chunks) - 1 and self.chunk_overlap > 0:
                next_chunk = chunks[i+1]
                overlap_text = next_chunk[:min(self.chunk_overlap, len(next_chunk))]
                current_chunk = current_chunk + overlap_text
            
            overlapped_chunks.append(current_chunk)
        
        return overlapped_chunks
    
    def split_by_sentences(self, text: str) -> List[str]:
        """
        æŒ‰å¥å­åˆ†å‰²ï¼ˆé€‚åˆä¸­è‹±æ–‡æ··åˆæ–‡æœ¬ï¼‰
        """
        # ä¸­è‹±æ–‡å¥å­åˆ†å‰²æ­£åˆ™è¡¨è¾¾å¼
        sentence_pattern = r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+'
        sentences = re.split(sentence_pattern, text)
        
        # åˆå¹¶çŸ­å¥å­
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += " " + sentence if current_chunk else sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks